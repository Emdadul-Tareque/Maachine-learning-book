{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# অধ্যায় ৬ঃ মেশিন লার্নিংয়ের জন্য দরকারি ফিচার সিলেক্ট করা "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "মেশিন লার্নি মডেলের  ভালো পারফর্মেন্সের জন্য দরকারি ফিচার অনেক বেশি গুরুত্বপূর্ণ। অদরকারি ফিচারের কারণে মডেলের পারফর্মেন্সের\n",
    "খারাপ হয়। তাই একটি ভালো প্রেডেক্টিভ মেশিন মডেল তৈরির জন্য ফিচার সিলেকশন করা গুরুত্বপূর্ণ। এই অধ্যায়ে আমরা ফিচার সিলেকশনের যে যে টেকনিকগুলো শিখব-- \n",
    "\n",
    "১) Univariate Selection.\n",
    "\n",
    "২) Recursive Feature Elimination.\n",
    "\n",
    "৩) Principle Component Analysis.\n",
    "\n",
    "৪) Feature Importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ৬.১ ফিচার সিলেকশন "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "যে ফিচারগুলো  টার্গেট ভ্যারিয়েবল প্রেডিক্ট করতে সাহায্য করে সেই ফিচারগুলোকে সিলেক্ট করাই হচ্ছে ফিচার সিলেকশন। তিনটি কারণে ফিচার সিলেকশন খুবই জরুরী। \n",
    "\n",
    "ওভারফিটিং কমানোঃ ওভারফিটিং মানে হচ্ছে মেশিন ট্রেনিং ডেটার উপর এমন ভালোভাবে শিখে যাকে মুখস্ত করার মত বলতে পারি যে মেশিন ট্রেনিং ডেটার উপর  ভালো পারফর্মেন্স করে কিন্তু টেস্ট ডেটার উপর পারফর্মেন্স খারাপ করে যা আমরা একমত চাই না । একই ধরনের ফিচার বারবার থাকলে এমনটা হয়। তাই ফিচার সিলেকশনের মাধ্যমে যদি ঐ ফিচার বাদ দেওয়া যায় তাহলে ওভারফিটিং কমবে।\n",
    "\n",
    "মডেলের একুরেসি বাড়ায়ঃ কম অদরকারি ডেটা মানে মেশিন ভালো ভাবে শিখতে পারে অর্থাৎ মডেলের একুরেসি বেশি হয়। \n",
    "\n",
    "মডেল ট্রেইন্ড হতে সময় কম লাগেঃ কম ডেটা মানে মেশিনকে ট্রেইন করাতে সময় কম লাগে। "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ৬.১.১ Univariate Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical tests এর মাধ্যমে যে ফিচারগুলো টার্গেট ভ্যারিয়েবলকে প্রেডিক্ট করার জন্য দায়ী সেগুলোকে সিলেক্ট করা যায়। The scikit-learn লাইব্রেরির SelectKBest ক্লাস বিভিন্ন Statistical tests এর মাধ্যমে ফিচার সিকেকশন করে । আমরা এখানে ch2 Statistical tests এর মাধ্যমে বেস্ট ৪টা ফিচার সিলেক্ট করব "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
      "[[148.    0.   33.6  50. ]\n",
      " [ 85.    0.   26.6  31. ]\n",
      " [183.    0.   23.3  32. ]\n",
      " [ 89.   94.   28.1  21. ]\n",
      " [137.  168.   43.1  33. ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "#load dataset\n",
    "dataframe = pd.read_csv('diabetes.csv')\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "# summarize scores\n",
    "set_printoptions(precision=3)\n",
    "scores = list(fit.scores_)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "# summarize selected features\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age'],\n",
      "      dtype='object')\n",
      "Pregnancies = 111.51969063588255\n",
      "Glucose = 1411.887040644141\n",
      "BloodPressure = 17.605373215320718\n",
      "SkinThickness = 53.10803983632434\n",
      "Insulin = 2175.5652729220137\n",
      "BMI = 127.669343331037\n",
      "DiabetesPedigreeFunction = 5.392681546971445\n",
      "Age = 181.30368904430023\n"
     ]
    }
   ],
   "source": [
    "feature_name = dataframe.columns\n",
    "i = 0\n",
    "for name in feature_name:\n",
    "    print(name, '=', scores[i])\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "আমরা এখানে দেখতে পাচ্ছি বেস্ট ৪টা ফিচার সিলেকশন স্কোর হচ্ছে যথাক্রমে Glucose, Insulin, BMI, Age।    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ৬.১.২ Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination টেকনিক হচ্ছে মেশিন লার্নিং এলগরিদম দিয়ে মেশিনকে ট্রেইন্ড করিয়ে যে যে ফিচার বা ফিচারগুলো টার্গেট ভ্যারিয়েবলকে প্রেডিক্ট করতে সাহায্য করে তাদেরকে চিহ্নিত করে এব যে যে ফিচার বা ফিচারগুলো টার্গেট ভ্যারিয়েবলকে প্রেডিক্ট করতে কম সাহায্য করে তাদেরকেও চিহ্নিত করে একটি লিস্ট করা হয় তারপর কম সাহায্যকারী ফিচারগুলোকে বাদ দিয়ে দেওয়া হয় সেই লিস্ট থেকে বের করে দেওয়া হয় ।  এই কাজটি Recursivly হয় বলে একে Recursive Feature Elimination বলা হয়।\n",
    "scikit learn এ RFE ক্লাস ব্যবহার করে এই কাজটি করা যায়। আমরা এখানে এলগরিদম হিসেবে logistics regression ব্যবহার করব। "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 4 5 6 1 1 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# feature extraction\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "আমারে এখানে দেখতে পাচ্ছি RFE তিনটি ফিচার সিলেক্ট করেছে Pregnancies, BMI, Age "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ৬.১.৩ Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (or PCA) লিনিয়ার এলজেবরা ব্যবহার করে ডেটাসেটকে কম্প্রেস করে। একে ডেটা  রিডাকশন টেকনিক বলা হয়ে থাকে। আমরা পরবর্তি কোন চ্যাপ্টারে এ ব্যপারে বিস্তারিত আলোচনা করব। এখানে PCA ব্যবহার করে ফিচার সিলেকশন দেখব। \n",
    "scikit learn এ PCA ক্লাস ব্যবহার করে ফিচার সিলেকশন টেকনিক ব্যবহার করা যায়। আমরা এখানে তিনটি দরকারি ফিচার সিলেক্ট করব। "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "x_pca = pca.transform(X)\n",
    "print(\"Before feature selection\", X.shape)\n",
    "print(\"After feature selection\", x_pca.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "আমরা দেখতে পাচ্ছি আমাদের ফিচার আগে ছিলো ৮ টি PCA করার পরার পর ফিচার হয়ে গেছে ৩ টি ।  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ৬.১.৪ Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagged decision trees যেমন  Random Forest and Extra Trees অথবা decission tree ব্যবহার করে গুরুতপূর্ন ফিচার বাঁচাই করতে পারি। \n",
    " আমরা এখানে ExtraTreesClassifier এবং DecisionTreeClassifier ব্যবহার ফিচার ইম্পোর্টেন্স দেখব। "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.109 0.235 0.099 0.08  0.076 0.14  0.12  0.14 ]\n",
      "Pregnancies 0.1093119514843894\n",
      "Glucose 0.23451534669524743\n",
      "BloodPressure 0.09943033455657531\n",
      "SkinThickness 0.08018295928945073\n",
      "Insulin 0.07641181870076626\n",
      "BMI 0.1403724420450305\n",
      "DiabetesPedigreeFunction 0.12003806683486525\n",
      "Age 0.1397370803936751\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "for name, score in zip(dataframe.columns,model.feature_importances_ ):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "আমরা এখানে বিভিন্ন ফিচারে স্কোর দেখতে পাচ্ছি। যে ফিচারের স্কোর বেশি তার গুরুত্ব বেশি। এখানে যথাক্রমে Glucose,BMI,Age এর গুরত্ব বেশি দেখতে পাচ্ছি।   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies 0.0\n",
      "Glucose 0.6564199687624899\n",
      "BloodPressure 0.0\n",
      "SkinThickness 0.0\n",
      "Insulin 0.0\n",
      "BMI 0.19253848413716962\n",
      "DiabetesPedigreeFunction 0.0\n",
      "Age 0.15104154710034046\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "model.fit(X, Y)\n",
    "for name, score in zip(dataframe.columns,model.feature_importances_ ):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier এর মাধ্যমেও দেখতে পাচ্ছি Glucose,BMI,Age এর গুরত্ব বেশি। "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ৬.২ অধ্যায় শেষে যা যা শিখলাম "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এই অধ্যায়ে মেশিন লার্নিং এর জন্য ডেটা প্রসেসিংয়ের আরেকটা ধাপ ফিচার সিলেকশন শিখে ফেললাম। আমরা ৪টা টেকনিক ব্যবহার করে ফিচার সিলেকশন করেছি -- \n",
    "১) Univariate Selection.\n",
    "\n",
    "২) Recursive Feature Elimination.\n",
    "\n",
    "৩) Principle Component Analysis.\n",
    "\n",
    "৪) Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ৬.৩ পরের অধ্যায়ে যা  শিখব "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এখন সময় এসেছে আমাদের ডেটা সেটে উপর  মেশিন লার্নিং এলগরিদম এভালুয়েট করার । পরের অধ্যায়ে আমরা ডেটা রিসেপ্লিং মেথড সম্পর্কে শিখব। ইনশাআল্লাহ।  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
